{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add some documentation on why we do what we do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add repo root directory to path. script.py and cleverparser.py are there\n",
    "import sys\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "\n",
    "from script0 import GetString, xml\n",
    "from cleverparser import CleverParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNPChunks(loc, strg):\n",
    "    NP_chunk = []\n",
    "    e = xml.etree.ElementTree.parse(loc)\n",
    "    dom = e.findall('org.apache.ctakes.typesystem.type.syntax.NP')\n",
    "    for atype in dom:\n",
    "        if atype.get('chunkType') == \"NP\":\n",
    "            start = int(atype.get('begin')) \n",
    "            end = int(atype.get('end'))\n",
    "            NP_chunk.append(strg[start : end])\n",
    "    return NP_chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing CleverParser\n",
    "First we load clever terms from `clever_base_terminology.txt`. Then we load the NP chunks from a given xml file. \n",
    "On each chunk, we search for clever terms, and replace them by the corresponding category tag (e.g. `scan` -> `SCREEN`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = CleverParser('../Data/clever_base_terminology.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = '../9.csv.xml'\n",
    "sofaString = GetString(file)\n",
    "NP_chunks = getNPChunks(file, sofaString)\n",
    "\n",
    "for chunk in NP_chunks:\n",
    "    newChunk = parser.trieReplacement(chunk)\n",
    "    print(chunk, '>', newChunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can do this for multiple files\n",
    "\n",
    "for file in glob('../Data/xml_reports/*.xml'):\n",
    "    sofaString = GetString(file)\n",
    "    NP_chunks = getNPChunks(file, sofaString)\n",
    "\n",
    "    print(file, '   ==================================')\n",
    "    for chunk in NP_chunks:\n",
    "        newChunk = parser.trieReplacement(chunk)\n",
    "        print(chunk, '>', newChunk)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
